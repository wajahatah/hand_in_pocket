{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Starting file analysis...\n",
      "\n",
      "‚úÖ Successfully analyzed data for 9 cameras.\n",
      "File saved to **C:/Users/LT/Downloads/video_inventory.csv**\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_video_sequences(folder_path, output_csv, video_extensions):\n",
    "    \"\"\"\n",
    "    Groups videos by Camera ID (c#), finds the sequence gaps (v#), \n",
    "    and saves the results to a structured CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Parse and Group Video Files\n",
    "    camera_data = defaultdict(list)\n",
    "    \n",
    "    # Regex to extract the camera ID (c#) and sequence number (v#)\n",
    "    # The 'r' ensures the string is treated as raw, and the parentheses create capture groups\n",
    "    pattern = re.compile(r'c(\\d+)_v(/d+)/.(?:' + '|'.join(ext.lstrip('.') for ext in video_extensions) + r')$', re.IGNORECASE)\n",
    "\n",
    "    print(\"üîé Starting file analysis...\")\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Full path is only needed to check if it's a file\n",
    "        full_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if os.path.isfile(full_path):\n",
    "            match = pattern.match(file_name)\n",
    "            \n",
    "            if match:\n",
    "                # Group 1 is the Camera ID (c#), Group 2 is the Sequence Number (v#)\n",
    "                camera_id = int(match.group(1))\n",
    "                video_seq = int(match.group(2))\n",
    "                \n",
    "                # Store the full file name and the sequence number\n",
    "                camera_data[camera_id].append({\n",
    "                    'name': file_name,\n",
    "                    'sequence': video_seq\n",
    "                })\n",
    "\n",
    "    if not camera_data:\n",
    "        print(\"‚ùå No matching video files found with the 'c#_v#' pattern.\")\n",
    "        return\n",
    "\n",
    "    # 2. Process Data: Sort, Find Gaps, and Prepare for CSV\n",
    "    \n",
    "    # Get sorted Camera IDs (e.g., [1, 2, 10, ...])\n",
    "    sorted_camera_ids = sorted(camera_data.keys())\n",
    "    \n",
    "    # This list will hold the final, structured rows for the CSV\n",
    "    csv_rows = []\n",
    "    \n",
    "    # Determine the maximum length needed for the column structure\n",
    "    max_len = 0\n",
    "    \n",
    "    # Process each camera ID to find gaps and sort videos\n",
    "    processed_data = {}\n",
    "    for cam_id in sorted_camera_ids:\n",
    "        videos = camera_data[cam_id]\n",
    "        \n",
    "        # Sort videos by their sequence number (v#)\n",
    "        videos.sort(key=lambda x: x['sequence'])\n",
    "        \n",
    "        # Extract sequences and find min/max\n",
    "        sequences = [v['sequence'] for v in videos]\n",
    "        min_seq = 1 # We assume the sequence starts at 1\n",
    "        max_seq = sequences[-1] if sequences else 0\n",
    "        \n",
    "        # Find missing numbers in the sequence\n",
    "        present_set = set(sequences)\n",
    "        all_expected_set = set(range(min_seq, max_seq + 1))\n",
    "        \n",
    "        missing_sequences = sorted(list(all_expected_set - present_set))\n",
    "        \n",
    "        # Store results\n",
    "        processed_data[cam_id] = {\n",
    "            'names': [v['name'] for v in videos],\n",
    "            'missing': [f\"v{m}\" for m in missing_sequences]\n",
    "        }\n",
    "        \n",
    "        # Update max length for proper column construction\n",
    "        max_len = max(max_len, len(videos) + len(processed_data[cam_id]['missing']) + 1)\n",
    "\n",
    "\n",
    "    # 3. Construct CSV Rows\n",
    "    \n",
    "    # Create the Header Row (e.g., ['C1 Video', 'C1 Missing', 'C2 Video', 'C2 Missing', ...])\n",
    "    header = []\n",
    "    for cam_id in sorted_camera_ids:\n",
    "        header.extend([f\"C{cam_id} Video Name\", f\"C{cam_id} Missing Sequence\"])\n",
    "    csv_rows.append(header)\n",
    "    \n",
    "    # Determine the max number of rows needed (names + missing) for the longest column\n",
    "    num_rows = 0\n",
    "    for cam_id in sorted_camera_ids:\n",
    "        # The list of entries to display for a camera is its names + its missing numbers\n",
    "        num_rows = max(num_rows, len(processed_data[cam_id]['names']) + len(processed_data[cam_id]['missing']))\n",
    "        \n",
    "    # Generate the Data Rows\n",
    "    for i in range(num_rows):\n",
    "        row = []\n",
    "        for cam_id in sorted_camera_ids:\n",
    "            data = processed_data[cam_id]\n",
    "            \n",
    "            # Combine the list of video names and the list of missing sequences\n",
    "            # Names go first, then missing sequences\n",
    "            combined_list = data['names'] + data['missing']\n",
    "            \n",
    "            # C# Video Name column:\n",
    "            video_name_entry = combined_list[i] if i < len(data['names']) else ''\n",
    "            row.append(video_name_entry)\n",
    "\n",
    "            # C# Missing Sequence column:\n",
    "            # Missing entries start where the video names end\n",
    "            missing_index = i - len(data['names'])\n",
    "            missing_entry = data['missing'][missing_index] if i >= len(data['names']) and missing_index < len(data['missing']) else ''\n",
    "            row.append(missing_entry)\n",
    "            \n",
    "        csv_rows.append(row)\n",
    "\n",
    "    # 4. Write the CSV File\n",
    "    try:\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_rows)\n",
    "        \n",
    "        print(f\"/n‚úÖ Successfully analyzed data for {len(sorted_camera_ids)} cameras.\")\n",
    "        print(f\"File saved to **{output_csv}**\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred while writing the CSV: {e}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. SET THE FOLDER PATH:\n",
    "VIDEO_FOLDER = \"C:/Users/LT/Downloads/fp/FP_S2\"\n",
    "\n",
    "# 2. SET THE OUTPUT FILE NAME:\n",
    "OUTPUT_FILE = \"C:/Users/LT/Downloads/video_inventory.csv\"\n",
    "\n",
    "# 3. DEFINE VIDEO EXTENSIONS:\n",
    "EXTENSIONS = ['.mp4', '.avi', '.mov', '.mkv'] \n",
    "\n",
    "# --- Run the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_video_sequences(VIDEO_FOLDER, OUTPUT_FILE, EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Starting total missing sequence analysis...\n",
      "  - C1: Found 137 sequences totally missing up to v267.\n",
      "  - C2: Found 152 sequences totally missing up to v279.\n",
      "  - C3: Found 182 sequences totally missing up to v280.\n",
      "  - C4: Found 149 sequences totally missing up to v329.\n",
      "  - C5: Found 140 sequences totally missing up to v304.\n",
      "  - C6: Found 140 sequences totally missing up to v271.\n",
      "  - C7: Found 195 sequences totally missing up to v281.\n",
      "  - C8: Found 168 sequences totally missing up to v260.\n",
      "  - C9: Found 207 sequences totally missing up to v257.\n",
      "  - C10: Found 202 sequences totally missing up to v271.\n",
      "\n",
      "‚úÖ Analysis complete. Results saved to **C:/Users/LT/Downloads/video_inventory_over.csv**\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_total_missing_sequences(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Finds sequence numbers (v#) that are absent from BOTH the 'Video Name' \n",
    "    and 'csvs' columns for each camera group, relative to the highest v# found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Input file not found at {input_csv}\")\n",
    "        return\n",
    "    \n",
    "    # Find all unique camera IDs (N)\n",
    "    camera_ids = sorted(\n",
    "        set(\n",
    "            int(re.search(r'C(/d+)', col).group(1))\n",
    "            for col in df.columns\n",
    "            if re.search(r'C(/d+) Video Name', col)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not camera_ids:\n",
    "        print(\"‚ùå Could not identify camera groups (C1, C2, etc.) in the columns.\")\n",
    "        return\n",
    "\n",
    "    video_pattern = re.compile(r'c/d+_v(/d+)(/..+)?$', re.IGNORECASE)\n",
    "    new_columns_data = {}\n",
    "    max_total_missing_len = 0 \n",
    "    \n",
    "    print(\"üîé Starting total missing sequence analysis...\")\n",
    "\n",
    "    for cam_id in camera_ids:\n",
    "        c_name_col = f'C{cam_id} Video Name'\n",
    "        c_csvs_col = f'C{cam_id} csvs'\n",
    "        new_total_missing_col = f'C{cam_id} Total Missing'\n",
    "        \n",
    "        # --- 1. Extract and Combine Sequences ---\n",
    "        \n",
    "        def extract_sequence(filename):\n",
    "            if pd.isna(filename):\n",
    "                return None\n",
    "            match = video_pattern.search(str(filename))\n",
    "            return int(match.group(1)) if match else None\n",
    "\n",
    "        names_sequences = set(df[c_name_col].apply(extract_sequence).dropna().unique())\n",
    "        csvs_sequences = set(df[c_csvs_col].apply(extract_sequence).dropna().unique())\n",
    "        \n",
    "        all_present_sequences = names_sequences.union(csvs_sequences)\n",
    "\n",
    "        # --- 2. Determine Range and Find Gaps ---\n",
    "        \n",
    "        max_seq = max(all_present_sequences) if all_present_sequences else 0\n",
    "        max_seq_int = int(max_seq) # Corrected TypeError\n",
    "        \n",
    "        expected_range = set(range(1, max_seq_int + 1))\n",
    "        \n",
    "        total_missing_sequences = sorted(list(expected_range - all_present_sequences))\n",
    "\n",
    "        # --- 3. Store Results ---\n",
    "        formatted_missing = [f\"v{seq}\" for seq in total_missing_sequences]\n",
    "        \n",
    "        new_columns_data[new_total_missing_col] = formatted_missing\n",
    "        max_total_missing_len = max(max_total_missing_len, len(formatted_missing))\n",
    "\n",
    "        print(f\"  - C{cam_id}: Found {len(formatted_missing)} sequences totally missing up to v{max_seq_int}.\")\n",
    "\n",
    "    # --- 3.5 PAD ALL NEW COLUMNS TO MAX LENGTH (THE NEW FIX) ---\n",
    "    for col_name, data_list in new_columns_data.items():\n",
    "        padding_needed = max_total_missing_len - len(data_list)\n",
    "        data_list.extend([''] * padding_needed)\n",
    "\n",
    "    # --- 4. Merge Results and Save CSV ---\n",
    "    \n",
    "    # 4a. Create the DataFrame for the new columns (succeeds due to padding)\n",
    "    new_df = pd.DataFrame(new_columns_data)\n",
    "    \n",
    "    # 4b. Determine the maximum required length for the final DataFrame\n",
    "    original_rows = len(df)\n",
    "    max_len_required = max(original_rows, max_total_missing_len)\n",
    "    \n",
    "    # 4c. Pad the ORIGINAL DataFrame (df) if it is shorter than the required length\n",
    "    if original_rows < max_len_required:\n",
    "        padding_rows = max_len_required - original_rows\n",
    "        padding_df_orig = pd.DataFrame('', \n",
    "                                     index=range(original_rows, max_len_required), \n",
    "                                     columns=df.columns)\n",
    "        df = pd.concat([df, padding_df_orig])\n",
    "    \n",
    "    # 4d. Pad the NEW DataFrame (new_df) if it is shorter than the required length\n",
    "    # This might seem redundant, but it guards against issues if max_total_missing_len was 0\n",
    "    # and original_rows > 0 (or vice-versa).\n",
    "    if len(new_df) < max_len_required:\n",
    "        padding_rows = max_len_required - len(new_df)\n",
    "        padding_df_new = pd.DataFrame('', \n",
    "                                      index=range(len(new_df), max_len_required), \n",
    "                                      columns=new_df.columns)\n",
    "        new_df = pd.concat([new_df, padding_df_new])\n",
    "        \n",
    "    # 4e. Concatenate horizontally (they now have the same length)\n",
    "    final_df = pd.concat([df.reset_index(drop=True), new_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Save the final DataFrame to a new CSV file\n",
    "    final_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    print(f\"/n‚úÖ Analysis complete. Results saved to **{output_csv}**\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "INPUT_FILE = \"C:/Users/LT/Downloads/video_inventory (1).csv\"  \n",
    "OUTPUT_FILE = \"C:/Users/LT/Downloads/video_inventory_over.csv\"\n",
    "\n",
    "# --- Run the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    find_total_missing_sequences(INPUT_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6197068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== TRAIN SET =====\n",
      "hand outside desk (ID 0): 86\n",
      "Hand on an arm rest (ID 1): 1793\n",
      "Hand in pocket (ID 2): 60\n",
      "Hand in pocket suspected (ID 3): 267\n",
      "Hand on desk (ID 4): 8083\n",
      "Pocket (ID 5): 60\n",
      "Hand on lap (ID 6): 402\n",
      "chair hand rest (ID 7): 8677\n",
      "Hand on head (ID 8): 1676\n",
      "hand under desk (ID 9): 289\n",
      "hand on back (ID 10): 39\n",
      "\n",
      "===== VAL SET =====\n",
      "hand outside desk (ID 0): 8\n",
      "Hand on an arm rest (ID 1): 578\n",
      "Hand in pocket (ID 2): 29\n",
      "Hand in pocket suspected (ID 3): 66\n",
      "Hand on desk (ID 4): 2402\n",
      "Pocket (ID 5): 29\n",
      "Hand on lap (ID 6): 215\n",
      "chair hand rest (ID 7): 2515\n",
      "Hand on head (ID 8): 406\n",
      "hand under desk (ID 9): 134\n",
      "hand on back (ID 10): 1\n",
      "\n",
      "===== TEST SET =====\n",
      "hand outside desk (ID 0): 0\n",
      "Hand on an arm rest (ID 1): 267\n",
      "Hand in pocket (ID 2): 6\n",
      "Hand in pocket suspected (ID 3): 42\n",
      "Hand on desk (ID 4): 1299\n",
      "Pocket (ID 5): 6\n",
      "Hand on lap (ID 6): 81\n",
      "chair hand rest (ID 7): 1510\n",
      "Hand on head (ID 8): 242\n",
      "hand under desk (ID 9): 43\n",
      "hand on back (ID 10): 6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# -------- CONFIG --------\n",
    "dataset_root = \"C:/wajahat/hand_in_pocket/dataset/images_bb/training2\"  # change this\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "\n",
    "# Example folder structure:\n",
    "# dataset_root/\n",
    "#   train/labels\n",
    "#   val/labels\n",
    "#   test/labels\n",
    "\n",
    "# If you want class names instead of IDs, define them here:\n",
    "class_names = {\n",
    "    0: \"hand outside desk\",\n",
    "    1: \"Hand on an arm rest\",\n",
    "    2: \"Hand in pocket\",\n",
    "    3: \"Hand in pocket suspected\",\n",
    "    4: \"Hand on desk\",\n",
    "    5: \"Pocket\",\n",
    "    6: \"Hand on lap\",\n",
    "    7: \"chair hand rest\",\n",
    "    8: \"Hand on head\",\n",
    "    9: \"hand under desk\",\n",
    "    10: \"hand on back\"\n",
    "}\n",
    "\n",
    "def count_instances(label_dir):\n",
    "    counts = defaultdict(int)\n",
    "\n",
    "    if not os.path.exists(label_dir):\n",
    "        print(f\"[WARNING] Missing folder: {label_dir}\")\n",
    "        return counts\n",
    "\n",
    "    for file in os.listdir(label_dir):\n",
    "        if not file.endswith(\".txt\"):\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(label_dir, file)\n",
    "\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip() == \"\":\n",
    "                    continue\n",
    "                class_id = int(line.split()[0])\n",
    "                counts[class_id] += 1\n",
    "\n",
    "    return counts\n",
    "\n",
    "def print_stats(split, counts):\n",
    "    print(f\"\\n===== {split.upper()} SET =====\")\n",
    "\n",
    "    if not counts:\n",
    "        print(\"No labels found.\")\n",
    "        return\n",
    "\n",
    "    max_id = max(max(counts.keys()), max(class_names.keys()))\n",
    "\n",
    "    for i in range(max_id + 1):\n",
    "        name = class_names.get(i, f\"class_{i}\")\n",
    "        print(f\"{name} (ID {i}): {counts.get(i, 0)}\")\n",
    "\n",
    "# -------- MAIN --------\n",
    "for split in splits:\n",
    "    label_path = os.path.join(dataset_root, \"labels\", split)\n",
    "    counts = count_instances(label_path)\n",
    "    print_stats(split, counts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
