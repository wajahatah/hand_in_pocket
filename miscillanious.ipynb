{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5d821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Starting file analysis...\n",
      "\n",
      "‚úÖ Successfully analyzed data for 9 cameras.\n",
      "File saved to **C:/Users/LT/Downloads/video_inventory.csv**\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def analyze_video_sequences(folder_path, output_csv, video_extensions):\n",
    "    \"\"\"\n",
    "    Groups videos by Camera ID (c#), finds the sequence gaps (v#), \n",
    "    and saves the results to a structured CSV.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Parse and Group Video Files\n",
    "    camera_data = defaultdict(list)\n",
    "    \n",
    "    # Regex to extract the camera ID (c#) and sequence number (v#)\n",
    "    # The 'r' ensures the string is treated as raw, and the parentheses create capture groups\n",
    "    pattern = re.compile(r'c(\\d+)_v(\\d+)\\.(?:' + '|'.join(ext.lstrip('.') for ext in video_extensions) + r')$', re.IGNORECASE)\n",
    "\n",
    "    print(\"üîé Starting file analysis...\")\n",
    "\n",
    "    # Iterate through all files in the directory\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        # Full path is only needed to check if it's a file\n",
    "        full_path = os.path.join(folder_path, file_name)\n",
    "        \n",
    "        if os.path.isfile(full_path):\n",
    "            match = pattern.match(file_name)\n",
    "            \n",
    "            if match:\n",
    "                # Group 1 is the Camera ID (c#), Group 2 is the Sequence Number (v#)\n",
    "                camera_id = int(match.group(1))\n",
    "                video_seq = int(match.group(2))\n",
    "                \n",
    "                # Store the full file name and the sequence number\n",
    "                camera_data[camera_id].append({\n",
    "                    'name': file_name,\n",
    "                    'sequence': video_seq\n",
    "                })\n",
    "\n",
    "    if not camera_data:\n",
    "        print(\"‚ùå No matching video files found with the 'c#_v#' pattern.\")\n",
    "        return\n",
    "\n",
    "    # 2. Process Data: Sort, Find Gaps, and Prepare for CSV\n",
    "    \n",
    "    # Get sorted Camera IDs (e.g., [1, 2, 10, ...])\n",
    "    sorted_camera_ids = sorted(camera_data.keys())\n",
    "    \n",
    "    # This list will hold the final, structured rows for the CSV\n",
    "    csv_rows = []\n",
    "    \n",
    "    # Determine the maximum length needed for the column structure\n",
    "    max_len = 0\n",
    "    \n",
    "    # Process each camera ID to find gaps and sort videos\n",
    "    processed_data = {}\n",
    "    for cam_id in sorted_camera_ids:\n",
    "        videos = camera_data[cam_id]\n",
    "        \n",
    "        # Sort videos by their sequence number (v#)\n",
    "        videos.sort(key=lambda x: x['sequence'])\n",
    "        \n",
    "        # Extract sequences and find min/max\n",
    "        sequences = [v['sequence'] for v in videos]\n",
    "        min_seq = 1 # We assume the sequence starts at 1\n",
    "        max_seq = sequences[-1] if sequences else 0\n",
    "        \n",
    "        # Find missing numbers in the sequence\n",
    "        present_set = set(sequences)\n",
    "        all_expected_set = set(range(min_seq, max_seq + 1))\n",
    "        \n",
    "        missing_sequences = sorted(list(all_expected_set - present_set))\n",
    "        \n",
    "        # Store results\n",
    "        processed_data[cam_id] = {\n",
    "            'names': [v['name'] for v in videos],\n",
    "            'missing': [f\"v{m}\" for m in missing_sequences]\n",
    "        }\n",
    "        \n",
    "        # Update max length for proper column construction\n",
    "        max_len = max(max_len, len(videos) + len(processed_data[cam_id]['missing']) + 1)\n",
    "\n",
    "\n",
    "    # 3. Construct CSV Rows\n",
    "    \n",
    "    # Create the Header Row (e.g., ['C1 Video', 'C1 Missing', 'C2 Video', 'C2 Missing', ...])\n",
    "    header = []\n",
    "    for cam_id in sorted_camera_ids:\n",
    "        header.extend([f\"C{cam_id} Video Name\", f\"C{cam_id} Missing Sequence\"])\n",
    "    csv_rows.append(header)\n",
    "    \n",
    "    # Determine the max number of rows needed (names + missing) for the longest column\n",
    "    num_rows = 0\n",
    "    for cam_id in sorted_camera_ids:\n",
    "        # The list of entries to display for a camera is its names + its missing numbers\n",
    "        num_rows = max(num_rows, len(processed_data[cam_id]['names']) + len(processed_data[cam_id]['missing']))\n",
    "        \n",
    "    # Generate the Data Rows\n",
    "    for i in range(num_rows):\n",
    "        row = []\n",
    "        for cam_id in sorted_camera_ids:\n",
    "            data = processed_data[cam_id]\n",
    "            \n",
    "            # Combine the list of video names and the list of missing sequences\n",
    "            # Names go first, then missing sequences\n",
    "            combined_list = data['names'] + data['missing']\n",
    "            \n",
    "            # C# Video Name column:\n",
    "            video_name_entry = combined_list[i] if i < len(data['names']) else ''\n",
    "            row.append(video_name_entry)\n",
    "\n",
    "            # C# Missing Sequence column:\n",
    "            # Missing entries start where the video names end\n",
    "            missing_index = i - len(data['names'])\n",
    "            missing_entry = data['missing'][missing_index] if i >= len(data['names']) and missing_index < len(data['missing']) else ''\n",
    "            row.append(missing_entry)\n",
    "            \n",
    "        csv_rows.append(row)\n",
    "\n",
    "    # 4. Write the CSV File\n",
    "    try:\n",
    "        with open(output_csv, mode='w', newline='', encoding='utf-8') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(csv_rows)\n",
    "        \n",
    "        print(f\"\\n‚úÖ Successfully analyzed data for {len(sorted_camera_ids)} cameras.\")\n",
    "        print(f\"File saved to **{output_csv}**\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå An error occurred while writing the CSV: {e}\")\n",
    "\n",
    "# --- Configuration ---\n",
    "# 1. SET THE FOLDER PATH:\n",
    "VIDEO_FOLDER = \"C:/Users/LT/Downloads/fp/FP_S2\"\n",
    "\n",
    "# 2. SET THE OUTPUT FILE NAME:\n",
    "OUTPUT_FILE = \"C:/Users/LT/Downloads/video_inventory.csv\"\n",
    "\n",
    "# 3. DEFINE VIDEO EXTENSIONS:\n",
    "EXTENSIONS = ['.mp4', '.avi', '.mov', '.mkv'] \n",
    "\n",
    "# --- Run the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_video_sequences(VIDEO_FOLDER, OUTPUT_FILE, EXTENSIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee3df15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Starting total missing sequence analysis...\n",
      "  - C1: Found 137 sequences totally missing up to v267.\n",
      "  - C2: Found 152 sequences totally missing up to v279.\n",
      "  - C3: Found 182 sequences totally missing up to v280.\n",
      "  - C4: Found 149 sequences totally missing up to v329.\n",
      "  - C5: Found 140 sequences totally missing up to v304.\n",
      "  - C6: Found 140 sequences totally missing up to v271.\n",
      "  - C7: Found 195 sequences totally missing up to v281.\n",
      "  - C8: Found 168 sequences totally missing up to v260.\n",
      "  - C9: Found 207 sequences totally missing up to v257.\n",
      "  - C10: Found 202 sequences totally missing up to v271.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All arrays must be of the same length",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 136\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;66;03m# --- Run the function ---\u001b[39;00m\n\u001b[32m    135\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     \u001b[43mfind_total_missing_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mfind_total_missing_sequences\u001b[39m\u001b[34m(input_csv, output_csv)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  - C\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcam_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(formatted_missing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m sequences totally missing up to v\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmax_seq_int\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# --- 4. Merge Results and Save CSV (THE FIX IS HERE) ---\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[38;5;66;03m# 4a. Create the DataFrame for the new columns\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m new_df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_columns_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# 4b. Determine the maximum required length for the final DataFrame\u001b[39;00m\n\u001b[32m     96\u001b[39m original_rows = \u001b[38;5;28mlen\u001b[39m(df)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LT\\.conda\\envs\\pose\\Lib\\site-packages\\pandas\\core\\frame.py:778\u001b[39m, in \u001b[36mDataFrame.__init__\u001b[39m\u001b[34m(self, data, index, columns, dtype, copy)\u001b[39m\n\u001b[32m    772\u001b[39m     mgr = \u001b[38;5;28mself\u001b[39m._init_mgr(\n\u001b[32m    773\u001b[39m         data, axes={\u001b[33m\"\u001b[39m\u001b[33mindex\u001b[39m\u001b[33m\"\u001b[39m: index, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m: columns}, dtype=dtype, copy=copy\n\u001b[32m    774\u001b[39m     )\n\u001b[32m    776\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[32m    777\u001b[39m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m     mgr = \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma.MaskedArray):\n\u001b[32m    780\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LT\\.conda\\envs\\pose\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:503\u001b[39m, in \u001b[36mdict_to_mgr\u001b[39m\u001b[34m(data, index, columns, dtype, typ, copy)\u001b[39m\n\u001b[32m    499\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    500\u001b[39m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[32m    501\u001b[39m         arrays = [x.copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mdtype\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[32m--> \u001b[39m\u001b[32m503\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LT\\.conda\\envs\\pose\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:114\u001b[39m, in \u001b[36marrays_to_mgr\u001b[39m\u001b[34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verify_integrity:\n\u001b[32m    112\u001b[39m     \u001b[38;5;66;03m# figure out the index, if necessary\u001b[39;00m\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         index = \u001b[43m_extract_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    116\u001b[39m         index = ensure_index(index)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\LT\\.conda\\envs\\pose\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:677\u001b[39m, in \u001b[36m_extract_index\u001b[39m\u001b[34m(data)\u001b[39m\n\u001b[32m    675\u001b[39m lengths = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(raw_lengths))\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(lengths) > \u001b[32m1\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mAll arrays must be of the same length\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    679\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m have_dicts:\n\u001b[32m    680\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    681\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mMixing dicts with non-Series may lead to ambiguous ordering.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    682\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: All arrays must be of the same length"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def find_total_missing_sequences(input_csv, output_csv):\n",
    "    \"\"\"\n",
    "    Finds sequence numbers (v#) that are absent from BOTH the 'Video Name' \n",
    "    and 'csvs' columns for each camera group, relative to the highest v# found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(input_csv)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"‚ùå Error: Input file not found at {input_csv}\")\n",
    "        return\n",
    "    \n",
    "    # Find all unique camera IDs (N)\n",
    "    camera_ids = sorted(\n",
    "        set(\n",
    "            int(re.search(r'C(\\d+)', col).group(1))\n",
    "            for col in df.columns\n",
    "            if re.search(r'C(\\d+) Video Name', col)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if not camera_ids:\n",
    "        print(\"‚ùå Could not identify camera groups (C1, C2, etc.) in the columns.\")\n",
    "        return\n",
    "\n",
    "    video_pattern = re.compile(r'c\\d+_v(\\d+)(\\..+)?$', re.IGNORECASE)\n",
    "    new_columns_data = {}\n",
    "    max_total_missing_len = 0 \n",
    "    \n",
    "    print(\"üîé Starting total missing sequence analysis...\")\n",
    "\n",
    "    for cam_id in camera_ids:\n",
    "        c_name_col = f'C{cam_id} Video Name'\n",
    "        c_csvs_col = f'C{cam_id} csvs'\n",
    "        new_total_missing_col = f'C{cam_id} Total Missing'\n",
    "        \n",
    "        # --- 1. Extract and Combine Sequences ---\n",
    "        \n",
    "        def extract_sequence(filename):\n",
    "            if pd.isna(filename):\n",
    "                return None\n",
    "            match = video_pattern.search(str(filename))\n",
    "            return int(match.group(1)) if match else None\n",
    "\n",
    "        names_sequences = set(df[c_name_col].apply(extract_sequence).dropna().unique())\n",
    "        csvs_sequences = set(df[c_csvs_col].apply(extract_sequence).dropna().unique())\n",
    "        \n",
    "        all_present_sequences = names_sequences.union(csvs_sequences)\n",
    "\n",
    "        # --- 2. Determine Range and Find Gaps ---\n",
    "        \n",
    "        max_seq = max(all_present_sequences) if all_present_sequences else 0\n",
    "        max_seq_int = int(max_seq) # Corrected TypeError\n",
    "        \n",
    "        expected_range = set(range(1, max_seq_int + 1))\n",
    "        \n",
    "        total_missing_sequences = sorted(list(expected_range - all_present_sequences))\n",
    "\n",
    "        # --- 3. Store Results ---\n",
    "        formatted_missing = [f\"v{seq}\" for seq in total_missing_sequences]\n",
    "        \n",
    "        new_columns_data[new_total_missing_col] = formatted_missing\n",
    "        max_total_missing_len = max(max_total_missing_len, len(formatted_missing))\n",
    "\n",
    "        print(f\"  - C{cam_id}: Found {len(formatted_missing)} sequences totally missing up to v{max_seq_int}.\")\n",
    "\n",
    "    # --- 3.5 PAD ALL NEW COLUMNS TO MAX LENGTH (THE NEW FIX) ---\n",
    "    for col_name, data_list in new_columns_data.items():\n",
    "        padding_needed = max_total_missing_len - len(data_list)\n",
    "        data_list.extend([''] * padding_needed)\n",
    "\n",
    "    # --- 4. Merge Results and Save CSV ---\n",
    "    \n",
    "    # 4a. Create the DataFrame for the new columns (succeeds due to padding)\n",
    "    new_df = pd.DataFrame(new_columns_data)\n",
    "    \n",
    "    # 4b. Determine the maximum required length for the final DataFrame\n",
    "    original_rows = len(df)\n",
    "    max_len_required = max(original_rows, max_total_missing_len)\n",
    "    \n",
    "    # 4c. Pad the ORIGINAL DataFrame (df) if it is shorter than the required length\n",
    "    if original_rows < max_len_required:\n",
    "        padding_rows = max_len_required - original_rows\n",
    "        padding_df_orig = pd.DataFrame('', \n",
    "                                     index=range(original_rows, max_len_required), \n",
    "                                     columns=df.columns)\n",
    "        df = pd.concat([df, padding_df_orig])\n",
    "    \n",
    "    # 4d. Pad the NEW DataFrame (new_df) if it is shorter than the required length\n",
    "    # This might seem redundant, but it guards against issues if max_total_missing_len was 0\n",
    "    # and original_rows > 0 (or vice-versa).\n",
    "    if len(new_df) < max_len_required:\n",
    "        padding_rows = max_len_required - len(new_df)\n",
    "        padding_df_new = pd.DataFrame('', \n",
    "                                      index=range(len(new_df), max_len_required), \n",
    "                                      columns=new_df.columns)\n",
    "        new_df = pd.concat([new_df, padding_df_new])\n",
    "        \n",
    "    # 4e. Concatenate horizontally (they now have the same length)\n",
    "    final_df = pd.concat([df.reset_index(drop=True), new_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Save the final DataFrame to a new CSV file\n",
    "    final_df.to_csv(output_csv, index=False, encoding='utf-8')\n",
    "    print(f\"\\n‚úÖ Analysis complete. Results saved to **{output_csv}**\")\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "\n",
    "INPUT_FILE = \"C:/Users/LT/Downloads/video_inventory (1).csv\"  \n",
    "OUTPUT_FILE = \"C:/Users/LT/Downloads/video_inventory_over.csv\"\n",
    "\n",
    "# --- Run the function ---\n",
    "if __name__ == \"__main__\":\n",
    "    find_total_missing_sequences(INPUT_FILE, OUTPUT_FILE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
